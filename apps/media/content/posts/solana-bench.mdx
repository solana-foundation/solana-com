---
slug: solana-bench
status: published
seo:
  title: "Introducing Solana Bench: How well can LLMs build complex transactions?"
  description: >-
    Learn about Solana Bench, a new tool for benchmarking LLMs knowledge about
    crypto transactions.
  noIndex: false
  noFollow: false
title: "Introducing Solana Bench: How well can LLMs build complex transactions?"
heroImage: /uploads/builder/solana-bench/2602a7b3aac1463fa30989b4cd876674.jpg
description: >
  Learn about Solana Bench, a new tool for benchmarking LLMs knowledge about
  crypto transactions.
author: content/authors/solana-foundation.md
date: 2025-09-03T00:00:00.000Z
categories:
  - category: content/categories/developers.mdx
tags:
  - tag: content/tags/product.mdx
  - tag: content/tags/case-studies.mdx
  - tag: content/tags/tech.mdx
cta: content/ctas/solana-bench.mdx
switchback: content/switchbacks/solana-bench.mdx
---

## Introducing Solana Bench

At the Solana Foundation, we want to fund open-source AI tooling that measurably improves how developers and applications use Solana. The challenge is measuring the usefulness of these tools. Until now, we haven't had a simple, reproducible way to evaluate whether new tools actually make it easier for language models to build and run transactions on Solana. We've experimented with Q&A benchmarks (too costly to maintain), tool-calling benchmarks in agent kits (too brittle and fragmented across stacks), and funding one-off toolkits (hard to track impact). Each attempt has taught us something, but none have given us a sustainable standard. That's why we're introducing [Solana Bench](https://solana-foundation.github.io/solana-gym-env/) â€” two lightweight, open-ended environments designed to test LLMs' operational competence on Solana in a way that is **simple, reproducible, and objective**.

1. **Basic** - maximize the number of **new instructions** successfully executed using only foundational SDKs (e.g. @solana/web3.js, Anchor, etc)
2. **Swap** - same success criterion, but within a Defi-leaning surface (Jupiter, Orca, Raydium, Phoenix, Meteora) using additional example prompts and preinstalled SDKs

These environments are not about measuring profit and loss. They are about **operational Solana competence**. These environments reward composing valid transactions, choosing accounts appropriately, using SDKs correctly, recovering from errors, and exploring breadth across programs. These environments are inspired by other open-ended benchmarks like [ClaudePlaysPokemon](https://www.anthropic.com/news/visible-extended-thinking), [TextQuest](https://huggingface.co/blog/textquests), and Nvidia's [Voyager](https://voyager.minedojo.org/).

![](/uploads/builder/solana-bench/image-1.jpg)

_Benchmark Solana operational competence trajectories measured by Solana Bench across common LLMs such as Claude-Sonnet-4, GPT-5, and Gemini-2.5-Flash._

## Start building with Solana Bench

Expand on this research! The Solana Foundation is funding open-sourced research on high-quality Solana LLM benchmarks.
